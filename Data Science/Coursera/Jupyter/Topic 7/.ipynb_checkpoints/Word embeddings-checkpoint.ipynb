{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557abce",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "\n",
    "## An alternative approach\n",
    "\n",
    "Can we defined words by the company they keep?\n",
    "\n",
    "\"If A and B have almost the identical environments we can say that they are synonyms\" (Zelig Harris, 1954)\n",
    "\n",
    "## Vector representations\n",
    "\n",
    "One-hot encodings are long and sparse\n",
    "\n",
    "Alternative: dense vectors\n",
    "\n",
    "short(length 50-1000) + dense(most elements are non-zero)\n",
    "\n",
    "## Benefits\n",
    "1. Easier to us in ML\n",
    "2. Offer better generalization capabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c28e9",
   "metadata": {},
   "source": [
    "## 1. Train your own emeddings\n",
    "\n",
    "Instead of counting terms, we trian a classifier on a prediction task:'does A occur near B'?\n",
    "\n",
    "The learnt classifier weights become our embeddings\n",
    "\n",
    "We can create our own word embeddings using gensim (an open-source library for unsupervised topic modeling and NLP) and train it on the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09454b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up libraries and data\n",
    "import gensim\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfaca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53fd58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy for later re-use\n",
    "model.save('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65df7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load models on demand\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c779bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15173"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfec48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11194711,  0.25647733,  0.20054246,  0.11136482, -0.06431483,\n",
       "       -0.33305392,  0.20328386,  0.34726298, -0.3063523 , -0.28288716,\n",
       "        0.16903785, -0.22773439,  0.19072987,  0.16677798,  0.24305744,\n",
       "       -0.16282238,  0.2695498 , -0.14498329, -0.52258486, -0.5172722 ,\n",
       "        0.28510782, -0.11505552,  0.48348287,  0.07632669, -0.05928324,\n",
       "       -0.13334103, -0.2300787 ,  0.01776804, -0.21009324,  0.23414841,\n",
       "        0.21649893, -0.06716539,  0.2807849 , -0.3703491 , -0.17710349,\n",
       "        0.05796826, -0.18897642, -0.05402145, -0.3442866 , -0.05958011,\n",
       "        0.01251105, -0.26838008,  0.17947957,  0.11442474,  0.2036804 ,\n",
       "       -0.00861567, -0.01929104, -0.02451881,  0.09921164,  0.31112176,\n",
       "        0.01644453, -0.28586334, -0.25123176, -0.18944709, -0.10734125,\n",
       "       -0.22981353,  0.17728397,  0.05186184, -0.06207724, -0.0792463 ,\n",
       "        0.02941441,  0.20066275, -0.03754928, -0.16876425, -0.18955061,\n",
       "        0.45259425,  0.02250968,  0.28408766, -0.2634104 ,  0.3700359 ,\n",
       "        0.20521757,  0.14640746,  0.21725862, -0.00294914,  0.33450156,\n",
       "        0.0896364 ,  0.10783947,  0.04687023,  0.0292988 , -0.12947907,\n",
       "       -0.11886341,  0.01182058, -0.21749416,  0.07352006, -0.1859877 ,\n",
       "       -0.01768531,  0.18966007,  0.0179308 ,  0.16606966,  0.07617442,\n",
       "        0.42552453, -0.1518142 , -0.12019853,  0.11286166,  0.32336387,\n",
       "        0.11950349,  0.1672189 , -0.36435127, -0.152315  ,  0.05187876],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "brown_model.wv['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01c44c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81546724"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similiarity between terms\n",
    "brown_model.wv.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "363fe0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9543903470039368),\n",
       " ('profession', 0.9534215331077576),\n",
       " ('neighborhood', 0.9529263973236084),\n",
       " ('congregation', 0.9514262676239014),\n",
       " ('selection', 0.948223888874054)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar terms\n",
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c00007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marble', 0.9657631516456604),\n",
       " ('pension', 0.9650222063064575),\n",
       " ('frankfurters', 0.9640059471130371),\n",
       " ('herd', 0.9625304937362671),\n",
       " ('towel', 0.9624120593070984)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('lemon',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c83d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.9278519749641418),\n",
       " ('policy', 0.9250001907348633),\n",
       " ('power', 0.923238217830658),\n",
       " ('Christian', 0.9232304096221924),\n",
       " ('education', 0.9201815128326416)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('government',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaba75",
   "metadata": {},
   "source": [
    "## 2. Use pre-trained embeddings\n",
    "\n",
    "We can load pre-built embeddings, e.g. a sample from a model trained on 100 billion words from the Google News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e78c976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to C:\\Users\\yi\n",
      "[nltk_data]     quan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58aeb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-build model\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6a53839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many terms?\n",
    "len(news_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "707dbfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "len(news_model['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d8ec4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003917098045349),\n",
       " ('faculty', 0.6780909895896912),\n",
       " ('undergraduate', 0.6587098240852356),\n",
       " ('campus', 0.6434985995292664),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they any better?\n",
    "news_model.most_similar(positive=['university'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6e01d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemons', 0.6462560296058655),\n",
       " ('apricot', 0.6199415326118469),\n",
       " ('avocado', 0.5922888517379761),\n",
       " ('fennel', 0.5873183012008667),\n",
       " ('coriander', 0.5828487873077393)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['lemon'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea47e46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Government', 0.7132058143615723),\n",
       " ('governments', 0.6521531343460083),\n",
       " ('administration', 0.546237051486969),\n",
       " ('legislature', 0.5307288765907288),\n",
       " ('parliament', 0.5268455147743225)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['government'],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915b3c1",
   "metadata": {},
   "source": [
    "## 3. Perform vector algebra\n",
    "\n",
    "We can use embeddings to perform verbal reasoning, e.g. A is to B as C is to....\n",
    "\n",
    "e.g. 'man is to king as woman is to...'\n",
    "\n",
    "vec(\"king\") - vec(\"man\") + vec(\"woman\") = ~ vec(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c52c8bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118194103240967),\n",
       " ('monarch', 0.6189676523208618),\n",
       " ('princess', 0.5902429819107056),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236845016479492)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['woman','king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e7c2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118194103240967),\n",
       " ('monarch', 0.6189676523208618),\n",
       " ('princess', 0.5902429819107056),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236845016479492)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['king','woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35b98c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091138839722),\n",
       " ('Belgium', 0.6197876334190369),\n",
       " ('Spain', 0.5664774179458618),\n",
       " ('Italy', 0.5654899477958679),\n",
       " ('Switzerland', 0.5609694123268127)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['Paris', 'Germany'], negative=['Berlin'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8848e25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shortest', 0.5145130157470703),\n",
       " ('steepest', 0.4244834780693054),\n",
       " ('first', 0.402511864900589),\n",
       " ('flattest', 0.40171924233436584),\n",
       " ('consecutive', 0.3951871693134308)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntatic patterns (verbs)\n",
    "news_model.most_similar(positive=['longest','short'],negative=['long'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a219a6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('purple', 0.5252775549888611),\n",
       " ('tulips', 0.49382397532463074),\n",
       " ('brown', 0.49077507853507996),\n",
       " ('pink', 0.4860530197620392),\n",
       " ('maroon', 0.4805646240711212)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more encyclopadeic knowledge\n",
    "news_model.most_similar(positive=['blue', 'tulip'], negative=['sky'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0d4608a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.8049386143684387),\n",
       " ('herself', 0.6881042718887329),\n",
       " ('me', 0.5886673927307129),\n",
       " ('She', 0.5803763270378113),\n",
       " ('woman', 0.5470801591873169)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic knowledge (pronouns)\n",
    "news_model.most_similar(positive=['him','she'], negative=['he'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8df517ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.40772414207458496),\n",
       " ('longer', 0.36700794100761414),\n",
       " ('lengthy', 0.3622959852218628),\n",
       " ('Long', 0.3600354790687561),\n",
       " ('continuous', 0.34982699155807495)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical knowledge (antonyms)\n",
    "news_model.most_similar(positive=['light','long'],negative=['dark'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdd1b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the add one out\n",
    "news_model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46cc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
